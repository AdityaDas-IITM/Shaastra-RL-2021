{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "according-recipient",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-seattle",
   "metadata": {},
   "source": [
    "## So what is Q learning?<br/>\n",
    "<font size = 4>Q learning is a method to evaluate how good it is to be in a state or how good it is to perform a certain action in a state based on the rewards the agent is receiving.<br/>\n",
    "\n",
    "Lets break this down. We will introduce two new terms, the state value function or V and the state-action value function or Q.<br/>\n",
    "    \n",
    "The State value function tells us how good it is for an agent to be in a particular state. This could be measured by calculating the average reward collected by the agent starting from that state till the end of the episode.<br/>\n",
    "Look at the example of a game given below.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-barbados",
   "metadata": {},
   "source": [
    "![](assets/grid1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-elder",
   "metadata": {},
   "source": [
    "<font size = 4>The goal here is for the agent to reach the end without falling in the Pit.\n",
    "Lets come up with a reward system.<br/>\n",
    "* If the agent falls in the Pit it gets a reward of -5 and the episode ends<br/>\n",
    "* If the agent reaches the end, it get a reward of +10 and the episode ends<br/>\n",
    "* For every step the agent takes, it gets a reward of -0.5. As the goal is to maximise the reward, this reward will encourage it to find the shortest and safest path to the end.<br/>\n",
    "\n",
    "Now intutively we can tell that the states around the pit must have a lesser value than the states around and close to the end.<br/>\n",
    "We can also tell that the states closer to the end must have a hugher value than the states far away. This is because of the -0.5 reward. This reward is basically telling that it would be preferred to be closer to the end as then we can reach the end faster and get lesser number of -0.5 rewards.<br/>\n",
    "So lets see a possible table of state values.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-music",
   "metadata": {},
   "source": [
    "![](assets/grid2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-planner",
   "metadata": {},
   "source": [
    "<font size = 4>Note that these values are just an example of what they could be. If we were given this table, the agent would know what action to take to rech the next best state and eventually the end.\n",
    "\n",
    "But in reality it is our work to find out this table. Lets see how to do that.</font>\n",
    "\n",
    "## Enter Bellman Equation\n",
    "\n",
    "### V(s) = R(s,a) + γ * V(s')\n",
    "<font size = 4>where<br/>\n",
    "- s is the curent state\n",
    "- a is the action\n",
    "- R is the reward you get for doing that action\n",
    "- s' is the next state\n",
    "- γ is the discount which basically decides the importance of future states when finding value of a state</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-darkness",
   "metadata": {},
   "source": [
    "## Q values\n",
    "<font size = 4>Now lets see what are Q values. It is often easier to compute and understand the quality of an action in a given state. That is exactly what Q values are. Q(s,a) Tells us how good it is to perfom action a when in state s.\n",
    "\n",
    "So our bellman equation for Q values is<br/></font>\n",
    "### Q(s,a) = R(s,a) + max<sub>a</sub>(Q(s', . ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-stock",
   "metadata": {},
   "source": [
    "<font size = 4>Once you have a QTable with a value for every (s,a) pair, you would choose the action that has the highest value in a given state and proceed to solve the game. So lets see how to use this equation to get a Qtable</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-consumer",
   "metadata": {},
   "source": [
    "## TD Error\n",
    "<font size = 4>You first begin by creating Qtable with all values as zeros or random values. Then you begin playing the game. You make an action a at a state s and get a reward R. So using the above equation you compute a value for Q(s,a). But in your table you already have some value for Q(s,a). The difference these two values is called the TD error. \n",
    "\n",
    "Once this error is computed the Qtable is updated with the following equation.<br/></font>\n",
    "### Q(s,a) = Q(s,a) + lr * TD error.\n",
    "<font size = 4>where lr is the learning rate. The reason for keeping a learning rate is because of stochasticity. Each time performing the (s,a) pair could lead to a net s' in the game so we do not want the qtbale to update all at once. Once this equation is run multiple times for all states, the qtable values converge to the actual values.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-uncle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
